{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Union\n",
    "\n",
    "class Tokenizer:\n",
    "\n",
    "    _WORD = re.compile(r'^\\w*\\b')\n",
    "\n",
    "    _LIBRARY = [' ', '\\n', '-- A comment', '.', 'abort', 'else', 'new', 'return', 'elsif', 'not', 'reverse', 'abstract', 'end', 'null', 'accept', 'entry', 'select', 'access', 'exception', 'of', 'separate', 'aliased', 'exit', 'some', 'all', 'others', 'subtype', 'and', 'for', 'out', 'synchronized', 'array', 'function', 'overriding', 'at', 'tagged', 'generic', 'package', 'task', 'begin', 'goto', 'pragma', 'terminate', 'body', 'private', 'then', 'if', 'procedure', 'type', 'case', 'in', 'protected', 'constant', 'interface', 'until', 'is', 'raise', 'use', 'declare', 'range', 'delay', 'limited', 'record', 'when', 'delta', 'loop', 'rem', 'while', 'digits', 'renames', 'with', 'do', 'mod', 'requeue', 'xor', 'abs', 'or', '=>', '(', ')', \"'\", '>=', '<=', '/=', '>', '<', ':=', '=', '+', '-', '*', '/', '**', '&', ',', ';', ':', '[', ']']\n",
    "    _LIBRARY_REGEX = {' ': re.compile(r'^ '), '\\n': re.compile(r'^(\\r)?\\n'), '-- A comment': re.compile(r'^--.*'), '.': re.compile(r'^\\.'), 'abort': re.compile(r'^\\babort\\b'), 'else': re.compile(r'^\\belse\\b'), 'new': re.compile(r'^\\bnew\\b'), 'return': re.compile(r'^\\breturn\\b'), 'elsif': re.compile(r'^\\belsif\\b'), 'not': re.compile(r'^\\bnot\\b'), 'reverse': re.compile(r'^\\breverse\\b'), 'abstract': re.compile(r'^\\babstract\\b'), 'end': re.compile(r'^\\bend\\b'), 'null': re.compile(r'^\\bnull\\b'), 'accept': re.compile(r'^\\baccept\\b'), 'entry': re.compile(r'^\\bentry\\b'), 'select': re.compile(r'^\\bselect\\b'), 'access': re.compile(r'^\\baccess\\b'), 'exception': re.compile(r'^\\bexception\\b'), 'of': re.compile(r'^\\bof\\b'), 'separate': re.compile(r'^\\bseparate\\b'), 'aliased': re.compile(r'^\\baliased\\b'), 'exit': re.compile(r'^\\bexit\\b'), 'some': re.compile(r'^\\bsome\\b'), 'all': re.compile(r'^\\ball\\b'), 'others': re.compile(r'^\\bothers\\b'), 'subtype': re.compile(r'^\\bsubtype\\b'), 'and': re.compile(r'^\\band\\b'), 'for': re.compile(r'^\\bfor\\b'), 'out': re.compile(r'^\\bout\\b'), 'synchronized': re.compile(r'^\\bsynchronized\\b'), 'array': re.compile(r'^\\barray\\b'), 'function': re.compile(r'^\\bfunction\\b'), 'overriding': re.compile(r'^\\boverriding\\b'), 'at': re.compile(r'^\\bat\\b'), 'tagged': re.compile(r'^\\btagged\\b'), 'generic': re.compile(r'^\\bgeneric\\b'), 'package': re.compile(r'^\\bpackage\\b'), 'task': re.compile(r'^\\btask\\b'), 'begin': re.compile(r'^\\bbegin\\b'), 'goto': re.compile(r'^\\bgoto\\b'), 'pragma': re.compile(r'^\\bpragma\\b'), 'terminate': re.compile(r'^\\bterminate\\b'), 'body': re.compile(r'^\\bbody\\b'), 'private': re.compile(r'^\\bprivate\\b'), 'then': re.compile(r'^\\bthen\\b'), 'if': re.compile(r'^\\bif\\b'), 'procedure': re.compile(r'^\\bprocedure\\b'), 'type': re.compile(r'^\\btype\\b'), 'case': re.compile(r'^\\bcase\\b'), 'in': re.compile(r'^\\bin\\b'), 'protected': re.compile(r'^\\bprotected\\b'), 'constant': re.compile(r'^\\bconstant\\b'), 'interface': re.compile(r'^\\binterface\\b'), 'until': re.compile(r'^\\buntil\\b'), 'is': re.compile(r'^\\bis\\b'), 'raise': re.compile(r'^\\braise\\b'), 'use': re.compile(r'^\\buse\\b'), 'declare': re.compile(r'^\\bdeclare\\b'), 'range': re.compile(r'^\\brange\\b'), 'delay': re.compile(r'^\\bdelay\\b'), 'limited': re.compile(r'^\\blimited\\b'), 'record': re.compile(r'^\\brecord\\b'), 'when': re.compile(r'^\\bwhen\\b'), 'delta': re.compile(r'^\\bdelta\\b'), 'loop': re.compile(r'^\\bloop\\b'), 'rem': re.compile(r'^\\brem\\b'), 'while': re.compile(r'^\\bwhile\\b'), 'digits': re.compile(r'^\\bdigits\\b'), 'renames': re.compile(r'^\\brenames\\b'), 'with': re.compile(r'^\\bwith\\b'), 'do': re.compile(r'^\\bdo\\b'), 'mod': re.compile(r'^\\bmod\\b'), 'requeue': re.compile(r'^\\brequeue\\b'), 'xor': re.compile(r'^\\bxor\\b'), 'abs': re.compile(r'^\\babs\\b'), 'or': re.compile(r'^\\bor\\b'), '=>': re.compile(r'^=>'), '(': re.compile(r'^\\('), ')': re.compile(r'^\\)'), \"'\": re.compile(r\"^'\"), '>=': re.compile(r'^>='), '<=': re.compile(r'^<='), '/=': re.compile(r'^/='), '>': re.compile(r'^>'), '<': re.compile(r'^<'), ':=': re.compile(r'^:='), '=': re.compile(r'^='), '+': re.compile(r'^\\+'), '-': re.compile(r'^-'), '*': re.compile(r'^\\*'), '/': re.compile(r'^/'), '**': re.compile(r'^\\*\\*'), '&': re.compile(r'^&'), ',': re.compile(r'^,'), ';': re.compile(r'^;'), ':': re.compile(r'^:'), '[': re.compile(r'^\\['), ']': re.compile(r'^\\]'),}\n",
    "\n",
    "    _STRING_LIT = 'STRING_LIT'\n",
    "\n",
    "    _LIBRARY = [_STRING_LIT] + _LIBRARY\n",
    "    _LIBRARY_REGEX[_STRING_LIT] = re.compile(r'^\"(\"\"|[^\"\\n])*\"')\n",
    "\n",
    "    _TOKEN_TO_ID = {k: v + 1 for v, k in enumerate(_LIBRARY)}\n",
    "    _ID_TO_TOKEN = {v: k for k, v in _TOKEN_TO_ID.items()}\n",
    "\n",
    "    _PAD = 0\n",
    "    _UKN1 = len(_LIBRARY) + 1\n",
    "    _ID_TO_TOKEN[_PAD] = ''\n",
    "    _ID_TO_TOKEN[_UKN1] = '#'\n",
    "\n",
    "\n",
    "    _NEWLINE_TOKEN = _TOKEN_TO_ID['\\n']\n",
    "    @classmethod\n",
    "    def newline_token(cls) -> int:\n",
    "        return cls._NEWLINE_TOKEN\n",
    "\n",
    "    _SPACE_TOKEN = _TOKEN_TO_ID[' ']\n",
    "    @classmethod\n",
    "    def space_token(cls) -> int:\n",
    "        return cls._SPACE_TOKEN\n",
    "\n",
    "    @classmethod\n",
    "    def _gen_uknown(cls, unknown_count: int) -> List[int]:\n",
    "        return [cls._UKN1] * unknown_count\n",
    "\n",
    "    @classmethod\n",
    "    def encode(cls, text: str) -> List[int]:\n",
    "        token_ids = []\n",
    "        unknown_count = 0\n",
    "        while text:\n",
    "            for token in cls._LIBRARY:\n",
    "                if match := cls._LIBRARY_REGEX[token].match(text):\n",
    "                    token_ids.extend(cls._gen_uknown(unknown_count))\n",
    "                    unknown_count = 0\n",
    "                    if token == cls._STRING_LIT:\n",
    "                        match_length = len(match.group())\n",
    "                        token_ids.extend([cls._TOKEN_TO_ID[token]] * match_length)\n",
    "                        text = text[match_length:]\n",
    "                    else:\n",
    "                        text = cls._LIBRARY_REGEX[token].sub('', text, count=1)\n",
    "                        token_ids.append(cls._TOKEN_TO_ID[token])\n",
    "                    break\n",
    "            else:\n",
    "                word_length = 1\n",
    "                if match := cls._WORD.match(text):\n",
    "                    word_length = len(match.group())\n",
    "                unknown_count += word_length\n",
    "                text = text[word_length:]\n",
    "        token_ids.extend(cls._gen_uknown(unknown_count))\n",
    "        return token_ids\n",
    "\n",
    "    @classmethod\n",
    "    def _decode_string_literals(cls, token_ids: List[int]) -> List[Union[int, str]]:\n",
    "        result = []\n",
    "        str_char_count = 0\n",
    "        for token_id in token_ids:\n",
    "            if token_id == cls._TOKEN_TO_ID[cls._STRING_LIT]:\n",
    "                str_char_count += 1\n",
    "            else:\n",
    "                if str_char_count > 0:\n",
    "                    result.append('\"' + cls._ID_TO_TOKEN[cls._UKN1] * (str_char_count - 2) + '\"')\n",
    "                    str_char_count = 0\n",
    "                result.append(token_id)\n",
    "        else:\n",
    "            if str_char_count > 0:\n",
    "                    result.append('\"' + cls._ID_TO_TOKEN[cls._UKN1] * (str_char_count - 2) + '\"')\n",
    "                    str_char_count = 0\n",
    "        return result\n",
    "\n",
    "    @classmethod\n",
    "    def decode(cls, token_ids: List[int]) -> str:\n",
    "        partial_decode = cls._decode_string_literals(token_ids)\n",
    "        text_parts = [cls._ID_TO_TOKEN[x] if isinstance(x, int) else x for x in partial_decode]\n",
    "        return ''.join(text_parts)\n",
    "\n",
    "    @classmethod\n",
    "    def resize(cls, token_ids: List[int], max_length: int) -> List[int]:\n",
    "        # If the token_ids are longer than max_length, truncate the start\n",
    "        # If the token_ids are shorter than max_length, pad the start with _PAD\n",
    "        if len(token_ids) > max_length:\n",
    "            return token_ids[-max_length:]\n",
    "        else:\n",
    "            return [cls._PAD] * (max_length - len(token_ids)) + token_ids\n",
    "\n",
    "    @classmethod\n",
    "    def n_vocab(cls) -> int:\n",
    "        return len(cls._ID_TO_TOKEN) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\"\"\n",
    "package body System.Text_IO is\n",
    "\n",
    "   ---------\n",
    "   -- Get --\n",
    "   ---------\n",
    "\n",
    "   function Get return Character is\n",
    "      Ret : constant Character :=\n",
    "        Character'Val (UART0_Periph.RXD.RXD and 16#FF#);\n",
    "        Ada.Text_IO.Put (\"Some text\");\n",
    "        Ada.Text_IO.Put (\"Some \"\"xt\");\n",
    "   begin\n",
    "\"\"\"\n",
    "\n",
    "tokens = Tokenizer.encode(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 39, 2, 45, 2, 101, 101, 101, 101, 101, 101, 5, 101, 101, 101, 101, 101, 101, 101, 2, 57, 3, 3, 2, 2, 2, 4, 3, 2, 2, 2, 4, 3, 2, 2, 2, 4, 3, 3, 2, 2, 2, 34, 2, 101, 101, 101, 2, 9, 2, 101, 101, 101, 101, 101, 101, 101, 101, 101, 2, 57, 3, 2, 2, 2, 2, 2, 2, 101, 101, 101, 2, 98, 2, 54, 2, 101, 101, 101, 101, 101, 101, 101, 101, 101, 2, 88, 3, 2, 2, 2, 2, 2, 2, 2, 2, 101, 101, 101, 101, 101, 101, 101, 101, 101, 82, 101, 101, 101, 2, 80, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 5, 101, 101, 101, 5, 101, 101, 101, 2, 29, 2, 101, 101, 101, 101, 101, 101, 81, 97, 3, 2, 2, 2, 2, 2, 2, 2, 2, 101, 101, 101, 5, 101, 101, 101, 101, 101, 101, 101, 5, 101, 101, 101, 2, 80, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 81, 97, 3, 2, 2, 2, 2, 2, 2, 2, 2, 101, 101, 101, 5, 101, 101, 101, 101, 101, 101, 101, 5, 101, 101, 101, 2, 80, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 81, 97, 3, 2, 2, 2, 41, 3]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 39, 2, 45, 2, 101, 101, 101, 101, 101, 101, 5, 101, 101, 101, 101, 101, 101, 101, 2, 57, 3, 3, 2, 2, 2, 4, 3, 2, 2, 2, 4, 3, 2, 2, 2, 4, 3, 3, 2, 2, 2, 34, 2, 101, 101, 101, 2, 9, 2, 101, 101, 101, 101, 101, 101, 101, 101, 101, 2, 57, 3, 2, 2, 2, 2, 2, 2, 101, 101, 101, 2, 98, 2, 54, 2, 101, 101, 101, 101, 101, 101, 101, 101, 101, 2, 88, 3, 2, 2, 2, 2, 2, 2, 2, 2, 101, 101, 101, 101, 101, 101, 101, 101, 101, 82, 101, 101, 101, 2, 80, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 5, 101, 101, 101, 5, 101, 101, 101, 2, 29, 2, 101, 101, 101, 101, 101, 101, 81, 97, 3, 2, 2, 2, 2, 2, 2, 2, 2, 101, 101, 101, 5, 101, 101, 101, 101, 101, 101, 101, 5, 101, 101, 101, 2, 80, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 81, 97, 3, 2, 2, 2, 2, 2, 2, 2, 2, 101, 101, 101, 5, 101, 101, 101, 101, 101, 101, 101, 5, 101, 101, 101, 2, 80, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 81, 97, 3, 2, 2, 2, 41, 3]\n"
     ]
    }
   ],
   "source": [
    "print(tokens)\n",
    "print(Tokenizer.resize(tokens, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "package body ######.####### is\n",
      "\n",
      "   -- A comment\n",
      "   -- A comment\n",
      "   -- A comment\n",
      "\n",
      "   function ### return ######### is\n",
      "      ### : constant ######### :=\n",
      "        #########'### (############.###.### and ######);\n",
      "        ###.#######.### (\"#########\");\n",
      "        ###.#######.### (\"#########\");\n",
      "   begin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = Tokenizer.decode(tokens)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
